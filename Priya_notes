In this Assignment, the goal is to find the number of clusters of a multivariate Gaussian mixture model for the dataset, and return the parameters. Recall that the joint distribution of the observed $\mathbf{x}$ is:$$
  f(\mathbf{x}|\theta) = \sum_{c=1}^{C}\pi_c f_1(\mathbf{x}|\mu_c, \Sigma_c)
  $$
    
    where $\mu_c \in \mathbb{R}^p$, $\Sigma_c \in \mathbb{R}^{p \times p}$ and $$
    f_c(\mathbf{x} \mid \mu_c, \sigma^2_c) = \left(\dfrac{1}{2\pi } \right)^{p/2} \dfrac{1}{|\Sigma_c|^{1/2}} \exp\left\{-\dfrac{(\mathbf{x}-\mu_c)^T \Sigma_c^{-1} (\mathbf{x} - \mu_c)}{2}  \right\}\,.
    $$
      
      The dataset is available here:
      
      ```{r}
    
    dat <- read.table("https://dvats.github.io/assets/course/mth210/MixG_data/221376.txt")
    dat <- as.matrix(dat)  # you will need to convert to matrix
    ```
    
 The data contains $X_1, X_2, \dots, X_{500}$ from a 4-dimensional mixture of Gaussian, where the number of clusters is either 2,3,4 or 5.
    
    Write a function that fits an EM algorithm and return MLE estimates of all parameters for a given cluster size. The last line of the function should be as indicated in the code below:
      
    
    GMMforD <- function(X, C = 2, tol = 1e-5, maxit = 1e3)
    {
      ....
      return(list("pi" = pi.list, "mu" = mu, "Sigma" = Sigma, "Ep" = Ep, "BIC" = BIC))
    }
    ```
    
    That is, the function should return a list with components:
      
      -   `pi` vector of estimates of $\pi_1, \pi_2, \dots, \pi_C$.
    
    -   `mu` list of estimates of $\mu_1, \mu_2, \dots, \mu_C$
      
      -   `Sigma` list of estimates of $\Sigma_1, \Sigma_2, \dots, \Sigma_C$
      
      -   `Ep` an $n \times C$ matrix of probabilities of each observation being assigned a cluster.
    
    -   `BIC` the BIC value using the above estimates.
    
    Using BIC as a criteria, find the best model for $C = 2,3,4,5$ and the corresponding estimates. Save your best model as `best.model`. This `best.model` should contain the output from the `GMMforD` function.
    







------------------------------------------------------------------------------------

GMMforD <- function(X, C = 2, tol = 1e-5, maxit = 1e3)
{
  n <- nrow(X)
  p <- ncol(X)
  
  # initialize parameters
  pi.list <- rep(1/C, C)
  mu <- list()
  Sigma <- list()
  for (c in 1:C) {
    mu[[c]] <- apply(X, 2, sample, 1)[,1]
    Sigma[[c]] <- cov(X)
  }
  Ep <- matrix(1/C, n, C)
  
  # EM algorithm
  for (iter in 1:maxit) {
    # E-step
    for (c in 1:C) {
      Ep[,c] <- pi.list[c] * dmvnorm(X, mu[[c]], Sigma[[c]])
    }
    Ep <- Ep / rowSums(Ep)
    
    # M-step
    Nk <- rowSums(Ep)
    pi.list <- Nk / n
    for (c in 1:C) {
      mu[[c]] <- colSums(X * Ep[,c]) / Nk[c]
      Sigma[[c]] <- (t(X) %*% (Ep[,c] * t(X))) / Nk[c] - outer(mu[[c]], mu[[c]])
    }
    
    # check convergence
    pi.diff <- max(abs(pi.list - pi.list.old))
    mu.diff <- max(sapply(1:C, function(c) max(abs(mu[[c]] - mu.old[[c]]))))
    Sigma.diff <- max(sapply(1:C, function(c) max(abs(Sigma[[c]] - Sigma.old[[c]]))))
    if (pi.diff < tol && mu.diff < tol && Sigma.diff < tol) {
      break
    }
    
    # update old parameters
    pi.list.old <- pi.list
    mu.old <- mu
    Sigma.old <- Sigma
  }
  
  # compute BIC
  loglik <- sum(log(apply(Ep, 1, function(ep) sum(pi.list * ep))))
  k <- C * (p*(p+1)/2 + 1) - 1
  BIC <- -2 * loglik + k * log(n)
  
  # return results
  return(list("pi" = pi.list, "mu" = mu, "Sigma" = Sigma, "Ep" = Ep, "BIC" = BIC))
}
